<!DOCTYPE html>
<html lang="en">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <meta name="generator" content="AsciiDoc 8.6.9">
      <title>HCI/CprE/ComS 575: Homework #6</title>
      <link rel="stylesheet" href="./riak.css" type="text/css">
   </head>

   <body class="article">
      <div id="header">
         <h1>HCI/CprE/ComS 575: Homework #6</h1>
         <!-- MAKE CHANGES HERE: Student information -->
         <span id="author">Your Name</span><br>
         <span id="email" class="monospaced">&lt;
         <a href="mailto:Your Email">Your Email</a>&gt;</span><br>
         <!-- END CHANGES -->
      </div>

      <div id="content">

	  <div id="preamble">
				<div class="sectionbody">
					<div class="paragraph">
						<p>
              The following libraries and references may be useful for solving this homework.
						<ul>
							<li class="level1">
								<div class="li"><a href="https://github.com/sukhoy/nanohmm"
                  class="urlextern" title="https://github.com/sukhoy/nanohmm"
                   rel="nofollow"> NanoHMM library</a> (includes both C and Python implementations).</div>
							</li>
              <li class="level1">
                <div class="li">
                  A tutorial on HMMs:
                  <a href="https://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf" class="urlextern" title="Tutorial on HMMs" rel="nofollow">
                  paper</a> and <a href="http://alumni.media.mit.edu/~rahimi/rabiner/rabiner-errata/rabiner-errata.html" class="urlextern" title="errata">errata</a>.
                </div>
              </li>
              <li>
                <div class="li">
                  <a href="https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm" class="urlextern" title="Forward-backward algorithm" rel="nofollow">
                  The Wikipedia article on the Forward-Backward algorithm.
                </a>
                </div>
              </li>
            </ul>
					</div>
				</div>
		</div>
		<hr>
		<br>

	     <!-- PART 1 -->
       <div class="sect1">
            <h2 id="_part_1">Part 1: Slow Forward Algorithm</h2>
            <div class="sectionbody">
               <div class="paragraph">
                  <p>Implement the &quot;slow&quot; version of the forward algorithm.
                    It should run in O(N<sup>T</sup>). It should support at least 4 states and sequences of length at least 5.
                    This should be your own code, i.e., you are not allowed to use any other libraries or implementations for this part.
                  </p>
               </div>
			   <div class="listingblock">
                  <div class="title">Source</div>
                  <div class="content monospaced">
                     <pre>
# This function implements the slow forward algorithm
def slowForwardAlgorithm(A, B, p, O):
      # Store necessary variables
      prob = 0
      numStates = len(A)
      pos = len(O)-1
      stateSeq = [0]*len(O)

      # Continue looping until all possible state sequences have been traversed
      while pos >= 0:
         curProb = p[stateSeq[0]] * B[stateSeq[0]][O[0]]
         for i in range(1,len(stateSeq)):
            curProb *= A[stateSeq[i-1]][stateSeq[i]] * B[stateSeq[i]][O[i]]
         prob += curProb

         if stateSeq[pos] < numStates-1:
            stateSeq[pos] += 1
         else:
            while stateSeq[pos] == numStates-1 and pos >= 0:
                  stateSeq[pos] = 0
                  pos -= 1
            if pos < 0:
                  break
            else:
                  stateSeq[pos] += 1
                  pos = len(O)-1

      return prob


# Compare the likelihood with my forward implementation (3A)
A = [[0.66, 0.34], [1.0, 0.0]]
B = [[0.5, 0.25, 0.25], [0.1, 0.1, 0.8]]
p = [0.8, 0.2]
O = [0, 1, 0, 2, 0, 1, 0]
result = slowForwardAlgorithm(A, B, p, O)
print(result)

# Compare the likelihood with my forward implementation (3B)
A = [[0.8, 0.1, 0.1],
      [0.4, 0.2, 0.4],
      [0, 0.3, 0.7]]
B = [[0.66, 0.34, 0],
      [0, 0, 1],
      [0.5, 0.4, 0.1]]
p = [0.6, 0, 0.4]
prob = slowForwardAlgorithm(A, B, p, O)
print("Likelihood = {}".format(prob))
					 </pre>
                  </div>
               </div>
</div>
</div>
		<hr>
		<br>


    <!-- PART 2 -->
         <div class="sect2">
            <h2 id="_part_2">Part 2: The Forward Algorithm</h2>
            <div class="sectionbody">
               <div class="paragraph">
                  <p>
                    Implement the Forward algorithm that runs in O(N<sup>2</sup>T).
                    It should support sequences of length at least 8 with at least 5 states. Because these numbers are relatively
                    small, your code doesn't have to re-normalize the probabilities at each step of the algorithm.
                    This should be your own code, i.e., you are not allowed to use any other libraries or implementations for this part.
                  </p>
               </div>
			   <div class="listingblock">
                  <div class="title">Source</div>
                  <div class="content monospaced">
                     <pre>
# This function implements the forward algorithm
def forwardAlgorithm(A, B, p, O):
   # Store useful variables for easier readability
   numStates = len(A)
   T = len(O)

   # 1) Initialization of alpha values
   alpha = []
   for i in range(0, numStates):
      alpha.append([])

   for i in range(0, numStates):
      alpha[i].append(p[i] * B[i][O[i]])

   # 2) Induction step
   for t in range(0, T-1):
      for j in range(0, numStates):
         alpha[j].append(0)
         for i in range(0, numStates):
               alpha[j][t+1] += alpha[i][t] * A[i][j]
         alpha[j][t+1] *= B[j][O[t+1]]

   # 3) Termination
   prob = 0;
   for i in range(0, numStates):
      prob += alpha[i][T-1]

   return alpha, prob


# Print results for Part 3A
A = [[0.66, 0.34], [1.0, 0.0]]
B = [[0.5, 0.25, 0.25], [0.1, 0.1, 0.8]]
p = [0.8, 0.2]
O = [0, 1, 0, 2, 0, 1, 0]
alpha, prob = forwardAlgorithm(A, B, p, O)
print("Part 3A Results")
print(str(alpha))
print("Likelihood = {}".format(prob))

# Print results for Part 3B
A = [[0.8, 0.1, 0.1],
   [0.4, 0.2, 0.4],
   [0, 0.3, 0.7]]
B = [[0.66, 0.34, 0],
   [0, 0, 1],
   [0.5, 0.4, 0.1]]
p = [0.6, 0, 0.4]
alpha, prob = forwardAlgorithm(A, B, p, O)
print("Part 3B Results")
print(str(alpha))
print("Likelihood = {}".format(prob))

# Print results for Part 6
O1 = [1, 0, 0, 0, 1, 0, 1]
O2 = [0, 0, 0, 1, 1, 2, 0]
O3 = [1, 1, 0, 1, 0, 1, 2]
O4 = [0, 1, 0, 2, 0, 1, 0]
O5 = [2, 2, 0, 1, 1, 0, 1]
A = [[0.6, 0.4],
   [1, 0]]
B = [[0.7, 0.3, 0],
   [0.1, 0.1, 0.8]]
pi = [0.7, 0.3]
alpha, prob = forwardAlgorithm(A, B, p, O1)
print("Likelihood for O1 = {}".format(prob))
alpha, prob = forwardAlgorithm(A, B, p, O2)
print("Likelihood for O2 = {}".format(prob))
alpha, prob = forwardAlgorithm(A, B, p, O3)
print("Likelihood for O3 = {}".format(prob))
alpha, prob = forwardAlgorithm(A, B, p, O4)
print("Likelihood for O4 = {}".format(prob))
alpha, prob = forwardAlgorithm(A, B, p, O5)
print("Likelihood for O5 = {}".format(prob))
					 </pre>
                  </div>
               </div>
</div>
</div>
		<hr>
		<br>


    <!-- PART 3 -->
    <div class="sect3">
       <h2 id="_part_3">Part 3: Forward Check</h2>
       <div class="sectionbody">
          <div class="paragraph">
             <p>
               Check your implementation of the forward algorithm by computing the forward variable alpha for
               the observation sequence O=(0,1,0,2,0,1,0) given the HMM.
             </p>
          </div>
          <div class="paragraph">
            <h3 id="_part_3a">Part 3A: Forward Check Using HMM with Two States</h3>
            <p>The HMM for Part 3A is specified below:
            <pre>
A = [[0.66, 0.34],
     [1, 0]]
B = [[0.5, 0.25, 0.25],
     [0.1, 0.1, 0.8]]
pi = [0.8, 0.2]
            </pre>
          </div>
          <div class="listingblock">
                   <div class="title">Result</div>
                   <div class="content monospaced">
                      <pre>
[[0.4, 0.07100000000000001, 0.030230000000000003, 0.00559145, 0.005956458500000001, 0.0010303429775000003, 0.00044127297707500016], 
   [0.020000000000000004, 0.013600000000000001, 0.0024140000000000008, 0.008222560000000002, 0.00019010930000000004, 0.00020251958900000005, 3.503166123500001e-05]]
                     </pre>
                   </div>
                </div>
          <div class="paragraph">
            <h3 id="_part_3b">Part 3B: Forward Check Using HMM with Three States</h3>
            <p>The HMM for Part 3B is specified below:
            <pre>
A = [[0.8, 0.1, 0.1],
     [0.4, 0.2, 0.4],
     [0, 0.3, 0.7]]
B = [[0.66, 0.34, 0],
     [0, 0, 1],
     [0.5, 0.4, 0.1]]
pi = [0.6, 0, 0.4]
            </pre>
          </div>
    <div class="listingblock">
             <div class="title">Result</div>
             <div class="content monospaced">
                <pre>
[[0.396, 0.10771200000000002, 0.05687193600000001, 0.0, 0.003919363430400001, 0.0010660668530688003, 0.0005628832984203267], 
   [0, 0.0, 0.0, 0.0148460736, 0.0, 0.0, 0.0], [0.2, 0.07184, 0.0305296, 0.0027057913600000002, 0.003916241696, 0.001253322212096, 0.00049196611688704]]
         </pre>
             </div>
          </div>
</div>
</div>
<hr>
<br>

        <!-- PART 4 -->
		<div class="sect4">
            <h2 id="_part_4">Part 4: The Backward Algorithm</h2>
            <div class="sectionbody">
                <div class="paragraph">
                  <p>Implement the Backward algorithm that runs in O(N<sup>2</sup>T).
                    It should support sequences of length at least 8 with at least 5 states. Because these numbers are relatively
                    small, your code doesn't have to re-normalize the probabilities at each step of the algorithm.
                    This should be your own code, i.e., you are not allowed to use any other libraries or implementations for this part.
				  </p>
                </div>
                <div class="listingblock">
                         <div class="title">Source</div>
                         <div class="content monospaced">
                            <pre>
# This function implements the backward algorithm
def backwardAlgorithm(A, B, O):
   # Store useful variables for easier readability
   numStates = len(A)
   T = len(O)

   # 1) Initialization of beta values
   beta = []
   for i in range(0, numStates):
      beta.append([])
      for j in range(0, T):
         beta[i].append(0)

   for i in range(0, numStates):
      beta[i][T-1] = 1

   # 2) Induction step
   for t in range(T-2, -1, -1):
      for i in range(0, numStates):
         for j in range(0, numStates):
               beta[i][t] += A[i][j] * B[j][O[t+1]] * beta[j][t+1]

   return beta


# Test part 5A
A = [[0.66, 0.34], [1.0, 0.0]]
B = [[0.5, 0.25, 0.25], [0.1, 0.1, 0.8]]
O = [0, 1, 0, 2, 0, 1, 0]
beta = backwardAlgorithm(A, B, O)
print("Part 5A Beta Values:")
print(str(beta))

# Test part 5B
A = [[0.8, 0.1, 0.1],
   [0.4, 0.2, 0.4],
   [0, 0.3, 0.7]]
B = [[0.66, 0.34, 0],
   [0, 0, 1],
   [0.5, 0.4, 0.1]]
beta = backwardAlgorithm(A, B, O)
print("Part 5B Beta Values:")
print(str(beta))                              
       					 </pre>
                         </div>
                      </div>
             </div>
  </div>
  <hr>
  <br>

  <!-- PART 5 -->
  <div class="sect5">
     <h2 id="_part_5">Part 5: Backward Check</h2>
     <div class="sectionbody">
        <div class="paragraph">
           <p>Check your implementation of the backward algorithm by computing the backward variable beta for
           the observation sequence O=(0,1,0,2,0,1,0) given the HMM.
           </p>
        </div>
        <div class="paragraph">
          <h3 id="_part_5a">Part 5A: Backward Check Using HMM with Two States</h3>
          <p>The HMM for Part 5A is specified below:
          <pre>
A = [[0.66, 0.34],
     [1, 0]]
B = [[0.5, 0.25, 0.25],
     [0.1, 0.1, 0.8]]
pi = [0.8, 0.2]
          </pre>
        </div>
        <div class="listingblock">
                 <div class="title">Result</div>
                 <div class="content monospaced">
                    <pre>
[[0.0011250862706500002, 0.005254026010000001, 0.015186587000000001, 0.028523800000000002, 0.07706, 0.364, 1], 
   [0.0013135065025000003, 0.007593293500000001, 0.0071309500000000005, 0.03853, 0.091, 0.5, 1]]
          </pre>
                 </div>
              </div>
        <div class="paragraph">
          <h3 id="_part_5b">Part 5B: Backward Check Using HMM with Three States</h3>
          <p>The HMM for Part 5B is specified below:
          <pre>
A = [[0.8, 0.1, 0.1],
     [0.4, 0.2, 0.4],
     [0, 0.3, 0.7]]
B = [[0.66, 0.34, 0],
     [0, 0, 1],
     [0.5, 0.4, 0.1]]
pi = [0.6, 0, 0.4]
          </pre>
        </div>
  <div class="listingblock">
           <div class="title">Result</div>
           <div class="content monospaced">
              <pre>
[[0.0015827267529984006, 0.004694663427200001, 0.006823102400000002, 0.09530204800000003, 0.17121600000000003, 0.5780000000000001, 1], [0.0018615874292992004, 0.006169560473600001, 0.014332204800000003, 0.06480102400000001, 0.134608, 0.464, 1], 
   [0.0021404481056, 0.007644457520000001, 0.021841307200000003, 0.0343, 0.09799999999999999, 0.35, 1]]
         </pre>
           </div>
        </div>
</div>
</div>
<hr>
<br>


<!-- PART 6 -->
<div class="sect6">
   <h2 id="_part_6">Part 6: Likelihood Calculation</h2>
   <div class="sectionbody">
      <div class="paragraph">
         <p>Compute the likelihood for each of the following five observation sequences given the same HMM model:
<pre>
O1 = (1,0,0,0,1,0,1)
O2 = (0,0,0,1,1,2,0)
O3 = (1,1,0,1,0,1,2)
O4 = (0,1,0,2,0,1,0)
O5 = (2,2,0,1,1,0,1)
</pre></p>
<p>The HMM for Part 6 is specified below:
<pre>
A = [[0.6, 0.4],
     [1, 0]]
B = [[0.7, 0.3, 0],
     [0.1, 0.1, 0.8]]
pi = [0.7, 0.3]
</pre></p>
<div class="paragraph"><p>
Hint: Compute this by adding the elements in the last column of the alpha array that is computed by your Forward algorithm.
</p></div></div>
<div class="listingblock">
         <div class="title">Result</div>
         <div class="content monospaced">
            <pre>
// Insert the computed likelihood for each sequence here.

Likelihood for O1 = 0.0004844856844799998
Likelihood for O2 = 0.0009391592448
Likelihood for O3 = 0.00013506766847999998
Likelihood for O4 = 0.0010799330304
Likelihood for O5 = 0.0
  </pre>
         </div>
      </div>
</div>
</div>
<hr>
<br>


<!-- PART 7 -->
<div class="sect7">
   <h2 id="_part_7">Part 7: Likelihood Verification</h2>
   <div class="sectionbody">
      <div class="paragraph">
         <p>
           Verify your implementations of the Forward algorithm and the Backward algorithm
           by computing the likelihood of the observation sequence in multiple ways.
           More specifically, show that the likelihood value can be computed by
           performing the dot product between the corresponding column of the
          forward array and the backward array for each t using the following HMM:
           <pre>
A = [[0.6, 0.4],
     [1, 0]]
B = [[0.7, 0.3, 0],
     [0.1, 0.1, 0.8]]
pi = [0.7, 0.3]
</pre></p>
<p>The observation sequences are:
<pre>
O1 = (1,0,0,0,1,0,1)
O2 = (0,0,0,1,1,2,0)
O3 = (1,1,0,1,0,1,2)
O4 = (0,1,0,2,0,1,0)
O5 = (2,2,0,1,1,0,1)
</pre></p></div>
<div class="listingblock">
         <div class="title">Result</div>
         <div class="content monospaced">
            <pre>
// Note: The HTML formatting may be messed up based on your browser size. Please look at the HTML code for the correct formatting
t=1							      t=2						      t=3						      t=4							   t=5						      t=6							     t=7
O1	L=1.9804932540839734e-06	L=4.137675122091958e-06	   L=8.666775100191128e-06	   L=1.7772485000417278e-05	L=7.695841798041598e-05	   L=0.00015939806561279996	  L=0.0006833869593599999
O2	L=2.6689264838183473e-06	L=5.563204456100657e-06	   L=1.1870736152002554e-05	L=4.812460602163198e-05	   L=0.0002673589223423999	   L=0.0005490406440959997	     L=0.0011935666175999994
O3	L=1.7460947698172562e-07	L=8.658993281708853e-07	   L=1.5622998259138562e-06	L=7.490478617395201e-06	   L=1.783447289856e-05	      L=0.0	                       L=0.00018577575935999999
O4	L=3.5297319894195885e-06	L=1.4722782809712226e-05	L=3.141539584540671e-05	   L=7.271375871836157e-05	   L=0.00015238696460287995	L=0.0006438321397759998	     L=0.0013537384447999997
O5	L=0.0	                     L=0.0	                     L=0.0	                     L=0.0	                     L=0.0	                     L=0.0	                       L=0.0
  </pre>
         </div>
      </div>
<div class="listingblock">
               <div class="title">Code</div>
               <div class="content monospaced">
                  <pre>
import numpy as np


# This function implements the forward algorithm
def forwardAlgorithm(A, B, p, O):
      # Store useful variables for easier readability
      numStates = len(A)
      T = len(O)

      # 1) Initialization of alpha values
      alpha = []
      for i in range(0, numStates):
         alpha.append([])

      for i in range(0, numStates):
         alpha[i].append(p[i] * B[i][O[i]])

      # 2) Induction step
      for t in range(0, T-1):
         for j in range(0, numStates):
            alpha[j].append(0)
            for i in range(0, numStates):
                  alpha[j][t+1] += alpha[i][t] * A[i][j]
            alpha[j][t+1] *= B[j][O[t+1]]

      # 3) Termination
      prob = 0;
      for i in range(0, numStates):
         prob += alpha[i][T-1]

      return alpha, prob


# This function implements the backward algorithm
def backwardAlgorithm(A, B, O):
      # Store useful variables for easier readability
      numStates = len(A)
      T = len(O)

      # 1) Initialization of beta values
      beta = []
      for i in range(0, numStates):
         beta.append([])
         for j in range(0, T):
            beta[i].append(0)

      for i in range(0, numStates):
         beta[i][T-1] = 1

      # 2) Induction step
      for t in range(T-2, -1, -1):
         for i in range(0, numStates):
            for j in range(0, numStates):
                  beta[i][t] += A[i][j] * B[j][O[t+1]] * beta[j][t+1]

      return beta


# Compute the Likelihood for each time segment for Part 7
O1 = [1, 0, 0, 0, 1, 0, 1]
O2 = [0, 0, 0, 1, 1, 2, 0]
O3 = [1, 1, 0, 1, 0, 1, 2]
O4 = [0, 1, 0, 2, 0, 1, 0]
O5 = [2, 2, 0, 1, 1, 0, 1]
A = [[0.6, 0.4],
      [1, 0]]
B = [[0.7, 0.3, 0],
      [0.1, 0.1, 0.8]]
p = [0.7, 0.3]
print("\tt=1\t\t\t\t\t\t\tt=2\t\t\t\t\t\tt=3\t\t\t\t\t\tt=4\t\t\t\t\t\t\tt=5\t\t\t\t\t\tt=6\t\t\t\t\t\t\tt=7")
alpha, prob = forwardAlgorithm(A, B, p, O1)
beta = backwardAlgorithm(A, B, O1)
likelihood = np.array(alpha).transpose().dot(np.array(beta))[len(O1)-1]
print("O1\tL={}\tL={}\tL={}\tL={}\tL={}\tL={}\tL={}".format(likelihood[0], likelihood[1], likelihood[2], likelihood[3], likelihood[4], likelihood[5], likelihood[6]))

alpha, prob = forwardAlgorithm(A, B, p, O2)
beta = backwardAlgorithm(A, B, O2)
likelihood = np.array(alpha).transpose().dot(np.array(beta))[len(O2)-1]
print("O2\tL={}\tL={}\tL={}\tL={}\tL={}\tL={}\tL={}".format(likelihood[0], likelihood[1], likelihood[2], likelihood[3], likelihood[4], likelihood[5], likelihood[6]))

alpha, prob = forwardAlgorithm(A, B, p, O3)
beta = backwardAlgorithm(A, B, O3)
likelihood = np.array(alpha).transpose().dot(np.array(beta))[len(O3)-1]
print("O3\tL={}\tL={}\tL={}\tL={}\tL={}\tL={}\tL={}".format(likelihood[0], likelihood[1], likelihood[2], likelihood[3], likelihood[4], likelihood[5], likelihood[6]))

alpha, prob = forwardAlgorithm(A, B, p, O4)
beta = backwardAlgorithm(A, B, O4)
likelihood = np.array(alpha).transpose().dot(np.array(beta))[len(O4)-1]
print("O4\tL={}\tL={}\tL={}\tL={}\tL={}\tL={}\tL={}".format(likelihood[0], likelihood[1], likelihood[2], likelihood[3], likelihood[4], likelihood[5], likelihood[6]))

alpha, prob = forwardAlgorithm(A, B, p, O5)
beta = backwardAlgorithm(A, B, O5)
likelihood = np.array(alpha).transpose().dot(np.array(beta))[len(O5)-1]
print("O5\tL={}\tL={}\tL={}\tL={}\tL={}\tL={}\tL={}".format(likelihood[0], likelihood[1], likelihood[2], likelihood[3], likelihood[4], likelihood[5], likelihood[6]))                     
        </pre>
               </div>
            </div>
</div>
</div>
<hr>
<br>

<!-- PART 8 -->
<div class="sect8">
   <h2 id="_part_8">Part 8: Match Sequences to HMMs</h2>
   <div class="sectionbody">
      <div class="paragraph">
         <p>Use your implementation of the Forward algorithm to compute the
            likelihood for each of the following five observation sequences given each
            of the following five HMMs. Fill the table below and indicate with *
            the most probable HMM for each sequence.
          </p>
        <p>The observation sequences are:
<pre>
O1 = (1,0,0,0,1,0,1)
O2 = (0,0,0,1,1,2,0)
O3 = (1,1,0,1,0,1,2)
O4 = (0,1,0,2,0,1,0)
O5 = (2,2,0,1,1,0,1)
</pre></p>
<p>The HMMs are:
<pre>
HMM 1:
A =  [[1.0, 0.0], [0.5, 0.5]]
B =  [[0.4, 0.6, 0.0], [0.0, 0.0, 1.0]]
pi =  [0.0, 1.0]

HMM 2:
A =  [[0.25, 0.75], [1.0, 0.0]]
B =  [[0, 1.0, 0], [0.66, 0.0, 0.34]]
pi =  [1.0, 0.0]

HMM 3:
A =  [[0.0, 1.0], [1.0, 0.0]]
B =  [[1.0, 0.0, 0.0], [0.0, 0.66, 0.34]]
pi =  [1.0, 0.0]

HMM 4:
A =  [[1, 0], [0.44, 0.56]]
B =  [[0.36, 0.42, 0.22], [1.0, 0, 0]]
pi =  [0, 1.0]

HMM 5:
A =  [[0.0, 1.0], [1.0, 0.0]]
B =  [[0.25, 0.75, 0.0], [1.0, 0.0, 0.0]]
pi =  [1.0, 0.0]
</pre>
</p>
      </div>
<div class="listingblock">
         <div class="title">Result</div>
         <div class="content monospaced">
            <pre>
// Note: The HTML formatting may be messed up based on your browser size. Please look at the HTML code for the correct formatting
   HMM1			      HMM2			      HMM3			   HMM4			               HMM5
O1	L=0.0				L=0.0				   L=0.0				L=0.01139308498944			*L=0.10546875				
O2	L=0.0				L=0.0				   L=0.0				*L=0.0039637063065600005	L=0.0				
O3	L=0.0				*L=0.01562034375	L=0.0				L=0.0				            L=0.0				
O4	L=0.0				L=0.0				   *L=0.148104		L=0.0				            L=0.0				
O5	*L=0.00864		L=0.0				   L=0.0				L=0.0				            L=0.0	
  </pre>
         </div>
      </div>
<div class="listingblock">
       <div class="title">Code</div>
           <div class="content monospaced">
    <pre>
import numpy as np


# This function implements the forward algorithm
def forwardAlgorithm(A, B, p, O):
      # Store useful variables for easier readability
      numStates = len(A)
      T = len(O)

      # 1) Initialization of alpha values
      alpha = []
      for i in range(0, numStates):
         alpha.append([])

      for i in range(0, numStates):
         alpha[i].append(p[i] * B[i][O[i]])

      # 2) Induction step
      for t in range(0, T-1):
         for j in range(0, numStates):
            alpha[j].append(0)
            for i in range(0, numStates):
                  alpha[j][t+1] += alpha[i][t] * A[i][j]
            alpha[j][t+1] *= B[j][O[t+1]]

      # 3) Termination
      prob = 0;
      for i in range(0, numStates):
         prob += alpha[i][T-1]

      return alpha, prob


# Compute the likelihood for each sequence
O1 = [1, 0, 0, 0, 1, 0, 1]
O2 = [0, 0, 0, 1, 1, 2, 0]
O3 = [1, 1, 0, 1, 0, 1, 2]
O4 = [0, 1, 0, 2, 0, 1, 0]
O5 = [2, 2, 0, 1, 1, 0, 1]
Os = [O1, O2, O3, O4, O5]
s1 = []
s2 = []
s3 = []
s4 = []
s5 = []
seqs = [s1, s2, s3, s4, s5]

# HMM 1:
A = [[1.0, 0.0], [0.5, 0.5]]
B = [[0.4, 0.6, 0.0], [0.0, 0.0, 1.0]]
p = [0.0, 1.0]
for i in range(0, len(Os)):
      alpha, prob = forwardAlgorithm(A, B, p, Os[i])
      seqs[i].append(prob)

# HMM 2:
A = [[0.25, 0.75], [1.0, 0.0]]
B = [[0, 1.0, 0], [0.66, 0.0, 0.34]]
p = [1.0, 0.0]
for i in range(0, len(Os)):
      alpha, prob = forwardAlgorithm(A, B, p, Os[i])
      seqs[i].append(prob)

# HMM 3:
A = [[0.0, 1.0], [1.0, 0.0]]
B = [[1.0, 0.0, 0.0], [0.0, 0.66, 0.34]]
p = [1.0, 0.0]
for i in range(0, len(Os)):
      alpha, prob = forwardAlgorithm(A, B, p, Os[i])
      seqs[i].append(prob)

# HMM 4:
A = [[1, 0], [0.44, 0.56]]
B = [[0.36, 0.42, 0.22], [1.0, 0, 0]]
p = [0, 1.0]
for i in range(0, len(Os)):
      alpha, prob = forwardAlgorithm(A, B, p, Os[i])
      seqs[i].append(prob)

# HMM 5:
A = [[0.0, 1.0], [1.0, 0.0]]
B = [[0.25, 0.75, 0.0], [1.0, 0.0, 0.0]]
p = [1.0, 0.0]
for i in range(0, len(Os)):
      alpha, prob = forwardAlgorithm(A, B, p, Os[i])
      seqs[i].append(prob)

# Determine max of each sequence
maxIndices = []
for seq in seqs:
      maxIndices.append(np.argmax(np.array(seq)))

# Print results
print("\tHMM1\t\t\tHMM2\t\t\tHMM3\t\t\tHMM4\t\t\tHMM5")
for i in range(0, len(seqs)):
      row = "O" + str(i+1) + "\t"
      for j in range(0, len(seqs[i])):
         if j == maxIndices[i]:
            row += "*"
         row += "L=" + str(seqs[i][j]) + "\t\t\t\t"
      print(row)      
</pre>
    </div>
    </div>
    </div>
  </div>
<hr>
<br>


<!-- PART 9 -->
<div class="sect9">
   <h2 id="_part_9">Part 9: Match Sequences to HMMs (using <a href="https://github.com/sukhoy/nanohmm" class="urlextern" title="https://github.com/sukhoy/nanohmm" rel="nofollow">NanoHMM</a>)</h2>
   <div class="sectionbody">
      <div class="paragraph">
         <p>
           This problem is similar to Part 8, but the sequences are now longer and
           your Forward and Backward algorithms may no longer work because they
           don't perform renormalization at each step.</p>
        <p>
           Use the implementation of the Forward algorithm in the <a href="https://github.com/sukhoy/nanohmm"
           class="urlextern" title="https://github.com/sukhoy/nanohmm" rel="nofollow">NanoHMM</a> library
           to compute the log-likelihood for each of the following five observation
           sequences given each of the following five HMMs. Fill the table below
           and indicate with * the most likely HMM for each sequence. In all cases,
           N=5, M=6, and T=20.
<pre>
O1 = (4,2,5,1,5,1,5,3,2,3,2,0,1,0,0,4,4,3,0,1)
O2 = (3,2,3,3,5,5,5,5,1,0,1,4,2,4,3,0,5,3,1,0)
O3 = (4,3,0,3,4,0,1,0,2,0,5,3,2,0,0,5,5,3,5,4)
O4 = (3,4,2,0,5,4,4,3,1,5,3,3,2,3,0,4,2,5,2,4)
O5 = (2,0,5,4,4,2,0,5,5,4,4,2,0,5,4,4,5,5,5,5)
</pre></p><p>The HMMs are:
<pre>
HMM 1:
A =  [[0.33, 0, 0, 0.67, 0],
      [0.67, 0, 0.33, 0, 0],
      [0, 1.0, 0.0, 0, 0],
      [0, 0, 0, 0.25, 0.75],
      [0.0, 0.0, 0.6, 0, 0.4]]
B =  [[0.67, 0, 0, 0, 0, 0.33],
      [0.0, 1.0, 0, 0, 0, 0],
      [0.5, 0, 0, 0, 0, 0.5],
      [0, 0, 0, 0.25, 0.75, 0],
      [0, 0.0, 0.6, 0.4, 0, 0.0]]
pi =  [0.0, 0.0, 0.0, 1.0, 0.0]


HMM 2:
A =  [[0.0, 0.0, 1.0, 0, 0.0],
      [0.0, 0, 0.0, 0.0, 1.0],
      [0.38, 0.0, 0.23, 0.38, 0.0],
      [0.0, 0.31, 0.0, 0.69, 0],
      [0.0, 0.75, 0.0, 0.25, 0.0]]
B =  [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0],
      [0.0, 0.6, 0.2, 0.2, 0.0, 0.0],
      [0.0, 0.0, 0, 1.0, 0.0, 0],
      [0, 0.0, 0, 0.22, 0.0, 0.78],
      [0.6, 0.0, 0.0, 0.0, 0.4, 0.0]]
pi =  [0.0, 0.0, 1.0, 0.0, 0.0]

HMM 3:
A =  [[0, 0.0, 0.32, 0.18, 0.5],
      [0.0, 0.0, 0.0, 1.0, 0.0],
      [0, 0.0, 0, 0.0, 1.0],
      [0, 0.64, 0, 0.0, 0.36],
      [1.0, 0.0, 0, 0, 0]]
B =  [[0.0, 0.17, 0.33, 0.0, 0.0, 0.5],
      [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],
      [0.47, 0.0, 0.0, 0.0, 0.0, 0.53],
      [0.27, 0.0, 0.0, 0.0, 0.73, 0.0],
      [0.66, 0.0, 0.0, 0.33, 0.0, 0.0]]
pi =  [0.0, 0.0, 0.0, 1.0, 0.0]

HMM 4:
A =  [[0.0, 0.0, 1.0, 0, 0.0],
      [0.0, 0, 0.62, 0, 0.38],
      [0.0, 0.5, 0.0, 0.5, 0.0],
      [0.0, 0.23, 0.0, 0.0, 0.77],
      [0.0, 0, 0, 1.0, 0]]
B =  [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0],
      [0.0, 0.0, 0.62, 0, 0.38, 0.0],
      [0, 0.0, 0.0, 0.0, 1, 0],
      [0, 0.0, 0, 0.41, 0.18, 0.41],
      [0.31, 0.16, 0.37, 0.16, 0, 0.0]]
pi =  [1.0, 0.0, 0.0, 0.0, 0]

HMM 5:
A =  [[0.5, 0.33, 0, 0.17, 0.0],
      [0.0, 0.0, 0.0, 0.0, 1.0],
      [0.75, 0.0, 0.25, 0.0, 0.0],
      [0.0, 0.0, 0, 1.0, 0.0],
      [0.0, 0.0, 1.0, 0.0, 0.0]]
B =  [[0.0, 0.0, 0.0, 0.0, 1.0, 0],
      [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],
      [0.0, 0.0, 0.0, 0.0, 0, 1.0],
      [0.0, 0.0, 0.0, 0.0, 0, 1.0],
      [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
pi =  [0.0, 1.0, 0.0, 0.0, 0.0]
</pre>
</p>
      </div>
<div class="listingblock">
         <div class="title">Result</div>
         <div class="content monospaced">
            <pre>
// Note: The HTML formatting may be messed up based on your browser size. Please look at the HTML code for the correct formatting
   HMM1			               HMM2			               HMM3			               HMM4			               HMM5
O1	*logL=-28.46471914737028	logL=-inf				      logL=-inf				      logL=-inf				   logL=-inf				
O2	logL=-inf				      *logL=-28.632927903652742	logL=-inf				      logL=-inf				      logL=-inf				
O3	logL=-inf				      logL=-inf				      *logL=-30.97288040871276	logL=-inf				      logL=-inf				
O4	logL=-inf				      logL=-inf				      logL=-inf				      *logL=-34.74407171447881	logL=-inf				
O5	logL=-inf				      logL=-inf				      logL=-inf				      logL=-inf				      *logL=-12.00042998719346	
  </pre>
         </div>
      </div>
      <div class="listingblock">
             <div class="title">Code</div>
                 <div class="content monospaced">
          <pre>
import nanohmm
import numpy as np

# Display observation sequences
O1 = [4, 2, 5, 1, 5, 1, 5, 3, 2, 3, 2, 0, 1, 0, 0, 4, 4, 3, 0, 1]
O2 = [3, 2, 3, 3, 5, 5, 5, 5, 1, 0, 1, 4, 2, 4, 3, 0, 5, 3, 1, 0]
O3 = [4, 3, 0, 3, 4, 0, 1, 0, 2, 0, 5, 3, 2, 0, 0, 5, 5, 3, 5, 4]
O4 = [3, 4, 2, 0, 5, 4, 4, 3, 1, 5, 3, 3, 2, 3, 0, 4, 2, 5, 2, 4]
O5 = [2, 0, 5, 4, 4, 2, 0, 5, 5, 4, 4, 2, 0, 5, 4, 4, 5, 5, 5, 5]
Os = [O1, O2, O3, O4, O5]
s1 = []
s2 = []
s3 = []
s4 = []
s5 = []
seqs = [s1, s2, s3, s4, s5]

# HMM 1:
A = [[0.33, 0, 0, 0.67, 0],
      [0.67, 0, 0.33, 0, 0],
      [0, 1.0, 0.0, 0, 0],
      [0, 0, 0, 0.25, 0.75],
      [0.0, 0.0, 0.6, 0, 0.4]]
B = [[0.67, 0, 0, 0, 0, 0.33],
      [0.0, 1.0, 0, 0, 0, 0],
      [0.5, 0, 0, 0, 0, 0.5],
      [0, 0, 0, 0.25, 0.75, 0],
      [0, 0.0, 0.6, 0.4, 0, 0.0]]
pi = [0.0, 0.0, 0.0, 1.0, 0.0]
for i in range(0, len(Os)):
      lambda_ = nanohmm.hmm_t(A, B, pi)
      f = nanohmm.forward_t(lambda_)
      LL = nanohmm.forward(f, Os[i])
      seqs[i].append(LL)

# HMM 2:
A = [[0.0, 0.0, 1.0, 0, 0.0],
      [0.0, 0, 0.0, 0.0, 1.0],
      [0.38, 0.0, 0.23, 0.38, 0.0],
      [0.0, 0.31, 0.0, 0.69, 0],
      [0.0, 0.75, 0.0, 0.25, 0.0]]
B = [[0.0, 0.0, 1.0, 0.0, 0.0, 0.0],
      [0.0, 0.6, 0.2, 0.2, 0.0, 0.0],
      [0.0, 0.0, 0, 1.0, 0.0, 0],
      [0, 0.0, 0, 0.22, 0.0, 0.78],
      [0.6, 0.0, 0.0, 0.0, 0.4, 0.0]]
pi = [0.0, 0.0, 1.0, 0.0, 0.0]
for i in range(0, len(Os)):
      lambda_ = nanohmm.hmm_t(A, B, pi)
      f = nanohmm.forward_t(lambda_)
      LL = nanohmm.forward(f, Os[i])
      seqs[i].append(LL)

# HMM 3:
A = [[0, 0.0, 0.32, 0.18, 0.5],
      [0.0, 0.0, 0.0, 1.0, 0.0],
      [0, 0.0, 0, 0.0, 1.0],
      [0, 0.64, 0, 0.0, 0.36],
      [1.0, 0.0, 0, 0, 0]]
B = [[0.0, 0.17, 0.33, 0.0, 0.0, 0.5],
      [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],
      [0.47, 0.0, 0.0, 0.0, 0.0, 0.53],
      [0.27, 0.0, 0.0, 0.0, 0.73, 0.0],
      [0.66, 0.0, 0.0, 0.33, 0.0, 0.0]]
pi = [0.0, 0.0, 0.0, 1.0, 0.0]
for i in range(0, len(Os)):
      lambda_ = nanohmm.hmm_t(A, B, pi)
      f = nanohmm.forward_t(lambda_)
      LL = nanohmm.forward(f, Os[i])
      seqs[i].append(LL)

# HMM 4:
A = [[0.0, 0.0, 1.0, 0, 0.0],
      [0.0, 0, 0.62, 0, 0.38],
      [0.0, 0.5, 0.0, 0.5, 0.0],
      [0.0, 0.23, 0.0, 0.0, 0.77],
      [0.0, 0, 0, 1.0, 0]]
B = [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0],
      [0.0, 0.0, 0.62, 0, 0.38, 0.0],
      [0, 0.0, 0.0, 0.0, 1, 0],
      [0, 0.0, 0, 0.41, 0.18, 0.41],
      [0.31, 0.16, 0.37, 0.16, 0, 0.0]]
pi = [1.0, 0.0, 0.0, 0.0, 0]
for i in range(0, len(Os)):
      lambda_ = nanohmm.hmm_t(A, B, pi)
      f = nanohmm.forward_t(lambda_)
      LL = nanohmm.forward(f, Os[i])
      seqs[i].append(LL)

# HMM 5:
A = [[0.5, 0.33, 0, 0.17, 0.0],
      [0.0, 0.0, 0.0, 0.0, 1.0],
      [0.75, 0.0, 0.25, 0.0, 0.0],
      [0.0, 0.0, 0, 1.0, 0.0],
      [0.0, 0.0, 1.0, 0.0, 0.0]]
B = [[0.0, 0.0, 0.0, 0.0, 1.0, 0],
      [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],
      [0.0, 0.0, 0.0, 0.0, 0, 1.0],
      [0.0, 0.0, 0.0, 0.0, 0, 1.0],
      [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]]
pi = [0.0, 1.0, 0.0, 0.0, 0.0]
for i in range(0, len(Os)):
      lambda_ = nanohmm.hmm_t(A, B, pi)
      f = nanohmm.forward_t(lambda_)
      LL = nanohmm.forward(f, Os[i])
      seqs[i].append(LL)

# Determine max of each sequence
maxIndices = []
for seq in seqs:
      maxIndices.append(np.argmax(np.array(seq)))

# Print results
print("\tHMM1\t\t\tHMM2\t\t\tHMM3\t\t\tHMM4\t\t\tHMM5")
for i in range(0, len(seqs)):
      row = "O" + str(i+1) + "\t"
      for j in range(0, len(seqs[i])):
         if j == maxIndices[i]:
            row += "*"
         row += "logL=" + str(seqs[i][j]) + "\t\t\t\t"
      print(row)            
      </pre>
          </div>
          </div>
</div>
</div>
<hr>
<br>

<!-- PART 10 -->
<div class="sect10">
   <h2 id="_part_10">Part 10: Train HMMs (using the <a href="https://github.com/sukhoy/nanohmm" class="urlextern" title="https://github.com/sukhoy/nanohmm" rel="nofollow">NanoHMM</a> library)</h2>
   <div class="sectionbody">
      <div class="paragraph">
        <p>The following five observation sequences are used for both parts 10A and 10B:
<pre>
O1 = (4,2,5,1,5,1,5,3,2,3,2,0,1,0,0,4,4,3,0,1)
O2 = (3,2,3,3,5,5,5,5,1,0,1,4,2,4,3,0,5,3,1,0)
O3 = (4,3,0,3,4,0,1,0,2,0,5,3,2,0,0,5,5,3,5,4)
O4 = (3,4,2,0,5,4,4,3,1,5,3,3,2,3,0,4,2,5,2,4)
O5 = (2,0,5,4,4,2,0,5,5,4,4,2,0,5,4,4,5,5,5,5)
</pre>
         </p>
      </div>
  <h3 id="_part_10a">Part 10A: Train 3-State HMMs</h3>
  <p>
    Train a 3-state HMM for each of the five observation sequences using the Baum-Welch
    implementation in the <a href="https://github.com/sukhoy/nanohmm"
    class="urlextern" title="https://github.com/sukhoy/nanohmm" rel="nofollow">NanoHMM</a> library.</p>
<div class="listingblock">
         <div class="title">Result</div>
         <div class="content monospaced">
            <pre>
// Note: The HTML formatting may be messed up based on your browser size. Please look at the HTML code for the correct formatting
Trained HMM 1 on sequence [4 2 5 1 5 1 5 3 2 3 2 0 1 0 0 4 4 3 0 1].
A[0][0] = 0.0000        A[0][1] = 0.0000        A[0][2] = 1.0000
A[1][0] = 0.0000        A[1][1] = 1.0000        A[1][2] = 0.0000
A[2][0] = 0.8333        A[2][1] = 0.1667        A[2][2] = 0.0000

B[0][0] = 0.0000        B[0][1] = 0.4000        B[0][2] = 0.2000        B[0][3] = 0.4000        B[0][4] = 0.0000        B[0][5] = 0.0000
B[1][0] = 0.4444        B[1][1] = 0.2222        B[1][2] = 0.0000        B[1][3] = 0.1111        B[1][4] = 0.2222        B[1][5] = 0.0000
B[2][0] = 0.0000        B[2][1] = 0.0000        B[2][2] = 0.3333        B[2][3] = 0.0000        B[2][4] = 0.1667        B[2][5] = 0.5000

pi[0]   = 0.0000        pi[1]   = 0.0000        pi[2]   = 1.0000

Trained HMM 2 on sequence [3 2 3 3 5 5 5 5 1 0 1 4 2 4 3 0 5 3 1 0].
A[0][0] = 0.0000        A[0][1] = 0.0000        A[0][2] = 1.0000
A[1][0] = 0.2857        A[1][1] = 0.7143        A[1][2] = 0.0000
A[2][0] = 0.8333        A[2][1] = 0.1667        A[2][2] = 0.0000

B[0][0] = 0.4286        B[0][1] = 0.0000        B[0][2] = 0.0000        B[0][3] = 0.2857        B[0][4] = 0.0000        B[0][5] = 0.2857
B[1][0] = 0.0000        B[1][1] = 0.0000        B[1][2] = 0.2857        B[1][3] = 0.4286        B[1][4] = 0.2857        B[1][5] = 0.0000
B[2][0] = 0.0000        B[2][1] = 0.5000        B[2][2] = 0.0000        B[2][3] = 0.0000        B[2][4] = 0.0000        B[2][5] = 0.5000

pi[0]   = 0.0000        pi[1]   = 1.0000        pi[2]   = 0.0000

Trained HMM 3 on sequence [4 3 0 3 4 0 1 0 2 0 5 3 2 0 0 5 5 3 5 4].
A[0][0] = 0.0000        A[0][1] = 0.8889        A[0][2] = 0.1111
A[1][0] = 0.8750        A[1][1] = 0.0000        A[1][2] = 0.1250
A[2][0] = 1.0000        A[2][1] = 0.0000        A[2][2] = 0.0000

B[0][0] = 0.4444        B[0][1] = 0.0000        B[0][2] = 0.0000        B[0][3] = 0.4444        B[0][4] = 0.0000        B[0][5] = 0.1111
B[1][0] = 0.2500        B[1][1] = 0.1250        B[1][2] = 0.2500        B[1][3] = 0.0000        B[1][4] = 0.0000        B[1][5] = 0.3750
B[2][0] = 0.0000        B[2][1] = 0.0000        B[2][2] = 0.0000        B[2][3] = 0.0000        B[2][4] = 1.0000        B[2][5] = 0.0000

pi[0]   = 0.0000        pi[1]   = 0.0000        pi[2]   = 1.0000

Trained HMM 4 on sequence [3 4 2 0 5 4 4 3 1 5 3 3 2 3 0 4 2 5 2 4].
A[0][0] = 0.0000        A[0][1] = 0.4286        A[0][2] = 0.5714
A[1][0] = 1.0000        A[1][1] = 0.0000        A[1][2] = 0.0000
A[2][0] = 0.0000        A[2][1] = 1.0000        A[2][2] = 0.0000

B[0][0] = 0.0000        B[0][1] = 0.0000        B[0][2] = 0.4286        B[0][3] = 0.4286        B[0][4] = 0.1429        B[0][5] = 0.0000
B[1][0] = 0.0000        B[1][1] = 0.0000        B[1][2] = 0.1250        B[1][3] = 0.0000        B[1][4] = 0.5000        B[1][5] = 0.3750
B[2][0] = 0.4000        B[2][1] = 0.2000        B[2][2] = 0.0000        B[2][3] = 0.4000        B[2][4] = 0.0000        B[2][5] = 0.0000

pi[0]   = 0.0000        pi[1]   = 0.0000        pi[2]   = 1.0000

Trained HMM 5 on sequence [2 0 5 4 4 2 0 5 5 4 4 2 0 5 4 4 5 5 5 5].
A[0][0] = 0.0000        A[0][1] = 1.0000        A[0][2] = 0.0000
A[1][0] = 0.0000        A[1][1] = 0.0000        A[1][2] = 1.0000
A[2][0] = 0.1538        A[2][1] = 0.0000        A[2][2] = 0.8462

B[0][0] = 0.0000        B[0][1] = 0.0000        B[0][2] = 1.0000        B[0][3] = 0.0000        B[0][4] = 0.0000        B[0][5] = 0.0000
B[1][0] = 1.0000        B[1][1] = 0.0000        B[1][2] = 0.0000        B[1][3] = 0.0000        B[1][4] = 0.0000        B[1][5] = 0.0000
B[2][0] = 0.0000        B[2][1] = 0.0000        B[2][2] = 0.0000        B[2][3] = 0.0000        B[2][4] = 0.4286        B[2][5] = 0.5714

pi[0]   = 1.0000        pi[1]   = 0.0000        pi[2]   = 0.0000               
  </pre>
         </div>
      </div>
          <h3 id="_part_10a">Part 10B: Train 4-State HMMs</h3>
          <p>
            Train a 4-state HMM for each of the five observation sequences using the Baum-Welch
            implementation in the <a href="https://github.com/sukhoy/nanohmm"
            class="urlextern" title="https://github.com/sukhoy/nanohmm" rel="nofollow">NanoHMM</a> library.</p>
        <div class="listingblock">
                 <div class="title">Result</div>
                 <div class="content monospaced">
                    <pre>
// Note: The HTML formatting may be messed up based on your browser size. Please look at the HTML code for the correct formatting
Trained HMM 1 on sequence [4 2 5 1 5 1 5 3 2 3 2 0 1 0 0 4 4 3 0 1].
A[0][0] = 0.0000        A[0][1] = 1.0000        A[0][2] = 0.0000        A[0][3] = 0.0000
A[1][0] = 0.4000        A[1][1] = 0.0000        A[1][2] = 0.6000        A[1][3] = 0.0000
A[2][0] = 0.3333        A[2][1] = 0.0000        A[2][2] = 0.0000        A[2][3] = 0.6667
A[3][0] = 0.0000        A[3][1] = 0.0000        A[3][2] = 1.0000        A[3][3] = 0.0000

B[0][0] = 0.2000        B[0][1] = 0.0000        B[0][2] = 0.0000        B[0][3] = 0.4000        B[0][4] = 0.4000        B[0][5] = 0.0000
B[1][0] = 0.0000        B[1][1] = 0.0000        B[1][2] = 0.6000        B[1][3] = 0.2000        B[1][4] = 0.2000        B[1][5] = 0.0000
B[2][0] = 0.5000        B[2][1] = 0.0000        B[2][2] = 0.0000        B[2][3] = 0.0000        B[2][4] = 0.0000        B[2][5] = 0.5000
B[3][0] = 0.0000        B[3][1] = 1.0000        B[3][2] = 0.0000        B[3][3] = 0.0000        B[3][4] = 0.0000        B[3][5] = 0.0000

pi[0]   = 1.0000        pi[1]   = 0.0000        pi[2]   = 0.0000        pi[3]   = 0.0000

Trained HMM 2 on sequence [3 2 3 3 5 5 5 5 1 0 1 4 2 4 3 0 5 3 1 0].
A[0][0] = 0.0000        A[0][1] = 0.3333        A[0][2] = 0.0000        A[0][3] = 0.6667
A[1][0] = 0.5000        A[1][1] = 0.5000        A[1][2] = 0.0000        A[1][3] = 0.0000
A[2][0] = 0.0000        A[2][1] = 0.1667        A[2][2] = 0.0000        A[2][3] = 0.8333
A[3][0] = 0.0000        A[3][1] = 0.0000        A[3][2] = 1.0000        A[3][3] = 0.0000

B[0][0] = 0.0000        B[0][1] = 0.0000        B[0][2] = 0.0000        B[0][3] = 1.0000        B[0][4] = 0.0000        B[0][5] = 0.0000
B[1][0] = 0.0000        B[1][1] = 0.0000        B[1][2] = 0.5000        B[1][3] = 0.0000        B[1][4] = 0.5000        B[1][5] = 0.0000
B[2][0] = 0.0000        B[2][1] = 0.5000        B[2][2] = 0.0000        B[2][3] = 0.0000        B[2][4] = 0.0000        B[2][5] = 0.5000
B[3][0] = 0.4286        B[3][1] = 0.0000        B[3][2] = 0.0000        B[3][3] = 0.2857        B[3][4] = 0.0000        B[3][5] = 0.2857

pi[0]   = 1.0000        pi[1]   = 0.0000        pi[2]   = 0.0000        pi[3]   = 0.0000

Trained HMM 3 on sequence [4 3 0 3 4 0 1 0 2 0 5 3 2 0 0 5 5 3 5 4].
A[0][0] = 0.0000        A[0][1] = 0.5000        A[0][2] = 0.0000        A[0][3] = 0.5000
A[1][0] = 1.0000        A[1][1] = 0.0000        A[1][2] = 0.0000        A[1][3] = 0.0000
A[2][0] = 0.0000        A[2][1] = 0.4000        A[2][2] = 0.0000        A[2][3] = 0.6000
A[3][0] = 0.0000        A[3][1] = 0.0000        A[3][2] = 1.0000        A[3][3] = 0.0000

B[0][0] = 1.0000        B[0][1] = 0.0000        B[0][2] = 0.0000        B[0][3] = 0.0000        B[0][4] = 0.0000        B[0][5] = 0.0000
B[1][0] = 0.0000        B[1][1] = 0.2500        B[1][2] = 0.5000        B[1][3] = 0.0000        B[1][4] = 0.2500        B[1][5] = 0.0000
B[2][0] = 0.0000        B[2][1] = 0.0000        B[2][2] = 0.0000        B[2][3] = 0.6667        B[2][4] = 0.1667        B[2][5] = 0.1667
B[3][0] = 0.3333        B[3][1] = 0.0000        B[3][2] = 0.0000        B[3][3] = 0.0000        B[3][4] = 0.1667        B[3][5] = 0.5000

pi[0]   = 0.0000        pi[1]   = 0.0000        pi[2]   = 0.0000        pi[3]   = 1.0000

Trained HMM 4 on sequence [3 4 2 0 5 4 4 3 1 5 3 3 2 3 0 4 2 5 2 4].
A[0][0] = 0.0000        A[0][1] = 1.0000        A[0][2] = 0.0000        A[0][3] = 0.0000
A[1][0] = 0.0000        A[1][1] = 0.0000        A[1][2] = 1.0000        A[1][3] = 0.0000
A[2][0] = 0.5714        A[2][1] = 0.0000        A[2][2] = 0.0000        A[2][3] = 0.4286
A[3][0] = 0.0000        A[3][1] = 0.0000        A[3][2] = 1.0000        A[3][3] = 0.0000

B[0][0] = 0.0000        B[0][1] = 0.0000        B[0][2] = 0.2500        B[0][3] = 0.7500        B[0][4] = 0.0000        B[0][5] = 0.0000
B[1][0] = 0.4000        B[1][1] = 0.2000        B[1][2] = 0.0000        B[1][3] = 0.4000        B[1][4] = 0.0000        B[1][5] = 0.0000
B[2][0] = 0.0000        B[2][1] = 0.0000        B[2][2] = 0.1250        B[2][3] = 0.0000        B[2][4] = 0.5000        B[2][5] = 0.3750
B[3][0] = 0.0000        B[3][1] = 0.0000        B[3][2] = 0.6667        B[3][3] = 0.0000        B[3][4] = 0.3333        B[3][5] = 0.0000

pi[0]   = 0.0000        pi[1]   = 1.0000        pi[2]   = 0.0000        pi[3]   = 0.0000

Trained HMM 5 on sequence [2 0 5 4 4 2 0 5 5 4 4 2 0 5 4 4 5 5 5 5].
A[0][0] = 0.5714        A[0][1] = 0.0000        A[0][2] = 0.0000        A[0][3] = 0.4286
A[1][0] = 1.0000        A[1][1] = 0.0000        A[1][2] = 0.0000        A[1][3] = 0.0000
A[2][0] = 0.0000        A[2][1] = 1.0000        A[2][2] = 0.0000        A[2][3] = 0.0000
A[3][0] = 0.1667        A[3][1] = 0.0000        A[3][2] = 0.3333        A[3][3] = 0.5000

B[0][0] = 0.0000        B[0][1] = 0.0000        B[0][2] = 0.0000        B[0][3] = 0.0000        B[0][4] = 0.0000        B[0][5] = 1.0000
B[1][0] = 1.0000        B[1][1] = 0.0000        B[1][2] = 0.0000        B[1][3] = 0.0000        B[1][4] = 0.0000        B[1][5] = 0.0000
B[2][0] = 0.0000        B[2][1] = 0.0000        B[2][2] = 1.0000        B[2][3] = 0.0000        B[2][4] = 0.0000        B[2][5] = 0.0000
B[3][0] = 0.0000        B[3][1] = 0.0000        B[3][2] = 0.0000        B[3][3] = 0.0000        B[3][4] = 1.0000        B[3][5] = 0.0000

pi[0]   = 0.0000        pi[1]   = 0.0000        pi[2]   = 1.0000        pi[3]   = 0.0000
               </pre>
                 </div>
              </div>
              <div class="listingblock">
                     <div class="title">Code</div>
                         <div class="content monospaced">
                  <pre>
// Copyright 2019 Vladimir Sukhoy and Alexander Stoytchev
//
// Permission is hereby granted, free of charge, to any person obtaining a copy
// of this software and associated documentation files (the "Software"), to deal
// in the Software without restriction, including without limitation the rights
// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
// copies of the Software, and to permit persons to whom the Software is
// furnished to do so, subject to the following conditions:
//
// The above copyright notice and this permission notice shall be included in
// all copies or substantial portions of the Software.

// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
// SOFTWARE.

// An example of how to use the nanohmm library.
#include "nanohmm.h"
#include <assert.h>
#include <math.h>
#include <stdarg.h>
#include <stdio.h>
#include <stdlib.h>

// print a message to the standard error stream
static int emsg(const char *fmt, ...) {
   fprintf(stderr, "[nanohmm example] ");
   va_list args;
   va_start(args, fmt);
   int rv = vfprintf(stderr, fmt, args);
   va_end(args);
   fprintf(stderr, "\n");
   return rv;
}

// normalize the array of nonnegative values x so that it sums up to 1
static void normalize(double *x, unsigned int n) {
   assert(n > 0);
   double sum = 0.0;
   unsigned int ix;
   for (ix=0; ix<n; ++ix) {
      assert(x[ix] >= 0.0);
      sum += x[ix];
   }
   for (ix=0; ix<n; ++ix)
      x[ix] = (sum == 0.0) ? (1.0/n):(x[ix]/sum);
}


// check the invariants
static void check_hmm_invariants(const hmm_t *h) {
   const unsigned int N = h->N;
   const unsigned int M = h->M;
   const double INVTOL = 0.0000001;

   unsigned int i,j;
   double sum = 0;
   for (i=0; i<N; ++i)
      sum += h->pi[i];
   if (fabs(sum - 1.0) > INVTOL) {
      emsg("Initial state probabilities (pi) do not sum up to 1.0. \
They sum up to %f. Terminating the program.", sum);
      exit(-1);
   }
   
   for (i=0; i<N; ++i) {
      sum = 0;
      for (j=0; j<N; ++j)
      sum += h->A[i][j];
      if (fabs(sum - 1.0) > INVTOL) {
      emsg("Transition probabilities for state %d (0-based) do not sum up to 1.0. \
They sum up to %f. Terminating the program.", i, sum);
      exit(-1);
      }
      sum = 0;
      for (j=0; j<M; ++j)
      sum += h->B[i][j];
      if (fabs(sum - 1.0) > INVTOL) {
      emsg("Observation probabilities for state %d (0-based) do not sum up to 1.0. \
They sum up to %f. Terminating the program.", i, sum);
      exit(-1);
      }
   }
}

// print the parameters (A, B, and pi) of the HMM
static void hmm_print(const hmm_t *h) {
   unsigned int ix, jx;
   for (ix=0; ix<h->N; ++ix) {
      for (jx=0; jx<h->N; ++jx)
      printf("A[%d][%d] = %.4f\t", ix, jx, h->A[ix][jx]);
      printf("\n");
   }
   printf("\n");
   for (ix=0; ix<h->N; ++ix) {
      for (jx=0; jx<h->M; ++jx)
      printf("B[%d][%d] = %.4f\t", ix, jx, h->B[ix][jx]);
      printf("\n");
   }
   printf("\n");
   for (ix=0; ix<h->N; ++ix)
      printf("pi[%d]   = %.4f\t", ix, h->pi[ix]);
   printf("\n");
}

// Train an HMM using the Baum-Welch algorithm and random restarts.
// It is assumed that the memory for the HMM is already fully allocated.
static double train(hmm_t *h, const unsigned int T, const unsigned int *O,
      unsigned int num_restarts, unsigned int num_iters) {
   const unsigned int N = h->N;
   const unsigned int M = h->M;

   assert(NULL != h && 0 < N && 0 < M && NULL != h->A && NULL != h->B && NULL != h->pi);
   double bestA[N][N];
   double bestB[N][M];
   double bestpi[N];
   double bestLL = -HUGE_VAL;
   unsigned int ix, jx;

   char buffer[baumwelch_block_size(N, M, T)];
   baumwelch_t *bw = baumwelch_init_block(buffer, h, T, 0);

   while (num_restarts--) {
      for (ix=0; ix<N; ++ix) {
      h->pi[ix] = (double)rand() / RAND_MAX;
      for (jx=0; jx<N; ++jx)
         h->A[ix][jx] = (double)rand() / RAND_MAX;
      normalize(h->A[ix], N);
      for (jx=0; jx<M; ++jx)
         h->B[ix][jx] = (double)rand() / RAND_MAX;
      normalize(h->B[ix], M);
      }
      normalize(h->pi, N);
      const double LL = baumwelch(bw, O, T, num_iters);
      if (LL > bestLL) {
      bestLL = LL;
      for (ix=0; ix<N; ++ix) {
         bestpi[ix] = h->pi[ix];
         for (jx=0; jx<N; ++jx)
            bestA[ix][jx] = h->A[ix][jx];
         for (jx=0; jx<M; ++jx)
            bestB[ix][jx] = h->B[ix][jx];
      }
      }
   }
   for (ix=0; ix<N; ++ix) {
      h->pi[ix] = bestpi[ix];
      for (jx=0; jx<N; ++jx)
      h->A[ix][jx] = bestA[ix][jx];
      for (jx=0; jx<M; ++jx)
      h->B[ix][jx] = bestB[ix][jx];
   }

   // validate the HMM parameters.
   unsigned int i, j;
   for (i=0; i<N; ++i) {
      double sum = 0;
      for (j=0; j<N; ++j)
      sum += h->A[i][j];

      if (sum < 0.000001) {
      emsg("Training produced a transition probability matrix (A) that contains a row of zeros. Patching it.");
      assert(sum >= 0.0);
      // Sometimes Baum-Welch may produce a matrix A with a zero row.
      // If this happens, then simply set one of the entries to 1.0 and the rest
      // to 0 so that the invariant is maintained.
      h->A[i][0] = 1.0;
      for (j=1; j<N; ++j)
         h->A[i][j] = 0.0;
      }
      check_hmm_invariants(h);
   }
   
   // Print the trained hmm
   hmm_print(h);
   
   return bestLL;
}


// compute the log-likelihood using forward-backward
static double compute_LL(const hmm_t *h, const unsigned int T, const unsigned int *O) {
   char f_block[forward_block_size(h->N, T)];  // allocate on stack
   forward_t *f = forward_init_block(f_block, h, T, 0);
   return forward(f, O, T);
}

// shows how to train an HMM from a fixed starting point without random restarts
static void bw_example() {
   printf("Baum-Welch example:\n");

   const unsigned int N = 2;
   const unsigned int M = 3;

   char hmm_block[hmm_block_size(N, M)];
   hmm_t *lambda = hmm_init_block(hmm_block, N, M);

   double **A = lambda->A;
   double **B = lambda->B;
   double *pi = lambda->pi;

   A[0][0] = 0.5;  A[0][1] = 0.5;
   A[1][0] = 0.0;  A[1][1] = 1.0;
   B[0][0] = 0.5;  B[0][1] = 0.5;  B[0][2] = 0.0;
   B[1][0] = 0.5;  B[1][1] = 0.0;  B[1][2] = 0.5;
   pi[0]   = 0.5;  pi[1]   = 0.5;

   const unsigned int O[] = {0, 1, 0, 2};
   const unsigned int T = sizeof(O) / sizeof(O[0]);

   printf("Running Baum-Welch from a fixed starting point.\n");

   char bw_buffer[baumwelch_block_size(N, M, T)];
   baumwelch_t *bw = baumwelch_init_block(bw_buffer, lambda, T, 0);

   const double LL = baumwelch(bw, O, T, 100);
   printf("Log-likelihood (base 2) of the sequence [");
   unsigned int ix;
   for (ix=0; ix<T; ++ix)
      printf("%d%s", O[ix], ix == T-1 ? "":" ");
   printf("] in the trained HMM is %g.\n", LL);
   printf("The parameters after training are:\n");
   hmm_print(lambda);
}

// shows training with random restarts and recognition using the trained HMMs
static void p10() {
   printf("Training HMMs...\n");
   const unsigned int example_O[5][20] = {{4,2,5,1,5,1,5,3,2,3,2,0,1,0,0,4,4,3,0,1},
                                       {3,2,3,3,5,5,5,5,1,0,1,4,2,4,3,0,5,3,1,0},
                                       {4,3,0,3,4,0,1,0,2,0,5,3,2,0,0,5,5,3,5,4},
                                       {3,4,2,0,5,4,4,3,1,5,3,3,2,3,0,4,2,5,2,4},
                              {2,0,5,4,4,2,0,5,5,4,4,2,0,5,4,4,5,5,5,5}};
   const unsigned int T = 20;
   const unsigned int ntrain = sizeof(example_O)/sizeof(example_O[0]);
   const unsigned int M = 6, N = 4;
   unsigned int train_ix, ix ,jx;
   hmm_t* h[ntrain];
   for (train_ix=0; train_ix<ntrain; ++train_ix) {
      h[train_ix] = malloc(hmm_block_size(N, M));
      hmm_init_block(h[train_ix], N, M);

   printf("\nTrained HMM %d on sequence [", train_ix + 1);
   for (jx=0; jx<T; ++jx)
      printf("%d%s", example_O[train_ix][jx], jx == T-1 ? "":" ");
   printf("].\n");
   
      train(h[train_ix], T, example_O[train_ix], 10, 50);
   }

   for (train_ix=0; train_ix<ntrain; ++train_ix)
      free(h[train_ix]);
}


static void viterbi_example() {
   printf("Viterbi example:\n");
   char hmm_block[hmm_block_size(2, 3)];
   hmm_t *h = hmm_init_block(hmm_block, 2, 3);

   h->A[0][0] = 0.25; h->A[0][1] = 0.75;
   h->A[1][0] = 0.2; h->A[1][1] = 0.8;

   h->B[0][0] = 0.65; h->B[0][1] = 0.2; h->B[0][2] = 0.15;
   h->B[1][0] = 0.21; h->B[1][1] = 0.29; h->B[1][2] = 0.5;

   h->pi[0] = 0.45; h->pi[1] = 0.55;

   check_hmm_invariants(h);

   const unsigned int O[] = {0, 1, 2};
   const unsigned int T = sizeof(O) / sizeof(O[0]);

   char viterbi_block[viterbi_block_size(2, T)];
   viterbi_t *v = viterbi_init_block(viterbi_block, h, T, 0);

   unsigned int Q[T];

   const double LL = viterbi(v, O, T, Q);

   printf("The most probable state sequence is: [");
   unsigned int ix;
   for (ix=0; ix<T; ++ix)
      printf("%d%s", Q[ix], ix == T-1 ? "":" ");
   printf("].\n");
   printf("Its probability is: %g.\n", pow(2, LL));
   printf("\n");
   printf("\n");
}


int main(int argc, const char* argv[]) {
   p10();
   return 0;
}                     
              </pre>
                  </div>
                  </div>
        </div>
  </div>
<hr>
<br>
        <h1 id="_ec">Extra Credit</h1>
        <div class="sectionbody">
           <div class="paragraph">
              <p>For each of the three problems below, you are allowed to use only
                your own code. In other words, you are not allowed to use any other
                 libraries or implementations for these problems.
              </p>
           </div>
         </div>
	     <!-- PART EC1 -->
         <div class="sectEC1">
            <h2 id="_part_ec1">Part EC1: Implement the Forward Algorithm with Re-Normalization</h2>
             <div class="listingblock">
                <div class="title">Source</div>
                <div class="content monospaced">
                  <pre>
// Insert your code here
                 </pre>
              </div>
          </div>
        </div>
			  <br>

        <!-- PART EC2 -->
          <div class="sectEC2">
             <h2 id="_part_ec2">Part EC2: Implement the Forward-Backward Algorithm with Re-Normalization</h2>
             <div class="listingblock">
                <div class="title">Source</div>
                <div class="content monospaced">
                  <pre>
// Insert your code here
                 </pre>
              </div>
          </div>
        </div>
 			  <br>

        <!-- PART EC3 -->
          <div class="sectEC3">
             <h2 id="_part_ec3">Part EC3: Implement the Baum-Welch Algorithm</h2>
             <div class="listingblock">
                <div class="title">Source</div>
                <div class="content monospaced">
                  <pre>
// Insert your code here
                 </pre>
              </div>
          </div>
        </div>
 			  <br>

      <div id="footer">
         <div id="footer-text">
            Last updated 2019-04-08
         </div>
      </div>
    </div>
   </body>
</html>
